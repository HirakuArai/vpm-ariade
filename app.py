"""
Optimized Streamlitâ€‘Kai entrypoint.
- Focuses on **minimal, deterministic** prompt generation.
- Always pulls the latest project rules / definition / architecture & DSL.
- Selfâ€‘check utilities live in `selfcheck.py` (not shown here).
"""
from __future__ import annotations

import json
import streamlit as st
from pathlib import Path
from textwrap import dedent

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ¨ Utility helpers
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

ROOT = Path(__file__).resolve().parent
DOCS = ROOT / "docs"
DSL  = ROOT / "dsl"


def read(path: str | Path) -> str:
    """Read UTFâ€‘8 text file."""
    return Path(path).read_text(encoding="utfâ€‘8")


def extract_section(md_path: Path, headings: list[str]) -> str:
    """Return Markdown for only the specified topâ€‘level headings.

    â–¸ `headings` is a list of H1/H2 text to keep.
    â–¸ Stops at the next heading of the same level.
    """
    lines = read(md_path).splitlines()
    keep, buf = False, []
    for line in lines:
        if line.startswith("#"):
            title = line.lstrip("# ").strip()
            keep = title in headings
        if keep:
            buf.append(line)
    return "\n".join(buf)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# ğŸ”‘  Prompt generator (single source of truth)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_system_prompt() -> str:
    """Return the **minimal & complete** system prompt for Kai."""

    # 1) Core operating rules
    rules = read(DOCS / "base_os_rules.md")

    # 2) Project definition & architecture (key sections only)
    proj = extract_section(DOCS / "project_definition.md", ["ç›®çš„", "ã‚´ãƒ¼ãƒ«", "Kai Support ã®å½¹å‰²"])
    arch = extract_section(DOCS / "architecture_overview.md", ["PoCæ§‹æˆ", "é‹ç”¨ãƒ•ãƒ­ãƒ¼"])

    # 3) DSL â€“ capabilities catalogue (name & description only)
    dsl_lines = []
    for raw in (DSL / "integrated_dsl.jsonl").read_text(encoding="utf-8").splitlines():
        try:
            item = json.loads(raw)
            name = item.get('name')
            desc = item.get('description')
            if not name or not desc:
                continue  # å¿…é ˆé …ç›®æ¬ è½ã¯ã‚¹ã‚­ãƒƒãƒ—
            dsl_lines.append(f"- **{name}**: {desc}")
        except Exception:
            continue  # ãƒ‘ãƒ¼ã‚¹ã§ããªã„è¡Œã‚‚ã‚¹ã‚­ãƒƒãƒ—
    dsl_block = "\n".join(dsl_lines)

    # 4) Compose
    prompt = dedent(
        f"""\
        ## Base Rules
        {rules}

        ## Project Overview
        {proj}

        ## Architecture Snapshot
        {arch}

        ## Kai Capabilities (DSL â€“ single source of truth)
        {dsl_block}
        """
    ).strip()

    return prompt


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Example usage inside Streamlit Kai (pseudoâ€‘code)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if __name__ == "__main__":  # local test
    print(get_system_prompt()[:1000])  # preview first 1k chars


# ãƒšãƒ¼ã‚¸è¨­å®š
st.set_page_config(page_title="Kai Chat", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Kai - GPTãƒãƒ£ãƒƒãƒˆ")

# å…¥åŠ›å±¥æ­´ã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ç®¡ç†
if "history" not in st.session_state:
    st.session_state["history"] = []

# ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›
user_input = st.text_input("ã‚ãªãŸã®ç™ºè¨€ï¼ˆé€ä¿¡ã§Enterï¼‰", "")

# ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆç”Ÿæˆ
system_prompt = get_system_prompt()

# å…¥åŠ›ãŒã‚ã‚Œã°é€ä¿¡
if user_input:
    import openai  # å¿…è¦ãªã‚‰å…ˆé ­ã§import
    # OpenAIã‚­ãƒ¼ã‚’ã©ã“ã‹ã§ã‚»ãƒƒãƒˆã™ã‚‹ã“ã¨
    messages = [{"role": "system", "content": system_prompt}]
    for msg in st.session_state["history"]:
        messages.append(msg)
    messages.append({"role": "user", "content": user_input})
    # GPT-4.1å‘¼ã³å‡ºã—ï¼ˆå¿…è¦ã«å¿œã˜ã¦APIã‚­ãƒ¼ã‚’ã‚»ãƒƒãƒˆï¼‰
    response = openai.chat.completions.create(
        model="gpt-4.1",
        messages=messages
    )
    reply = response.choices[0].message.content
    st.session_state["history"].append({"role": "user", "content": user_input})
    st.session_state["history"].append({"role": "assistant", "content": reply})
    st.experimental_rerun()  # ãƒšãƒ¼ã‚¸ã‚’å†æç”»

# å±¥æ­´è¡¨ç¤º
for msg in st.session_state["history"]:
    is_user = msg["role"] == "user"
    st.chat_message("user" if is_user else "assistant").markdown(msg["content"])
