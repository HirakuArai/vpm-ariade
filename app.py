"""app.py â€“ Kai (Minimal GPT Chat with Robust Prompt, OpenAIÂ v1.x)"""

# â”€â”€ Imports & Setup â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
from __future__ import annotations
import os, json, sys, traceback
from pathlib import Path
from textwrap import dedent
import streamlit as st
import openai  # openai-python >=1.1.0 ã«å¯¾å¿œ

# .env èª­ã¿è¾¼ã¿ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«é–‹ç™ºç”¨ï¼‰
try:
    from dotenv import load_dotenv
    load_dotenv()
except ImportError:
    pass

# API ã‚­ãƒ¼è¨­å®šï¼ˆsecrets.toml > ç’°å¢ƒå¤‰æ•°ï¼‰
openai.api_key = os.getenv("OPENAI_API_KEY", st.secrets.get("OPENAI_API_KEY", ""))
if not openai.api_key:
    st.error("âŒ OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")

ROOT = Path(__file__).resolve().parent
DOCS = ROOT / "docs"
DSL  = ROOT / "dsl"

# â”€â”€ Utilities â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def read(path: Path | str) -> str:
    return Path(path).read_text(encoding="utf-8")

def extract_section(md_path: Path, headings: list[str]) -> str:
    """Markdownã‹ã‚‰æŒ‡å®šè¦‹å‡ºã—ã®ã¿æŠ½å‡º"""
    lines = read(md_path).splitlines()
    keep, buf = False, []
    for line in lines:
        if line.startswith("#"):
            title = line.lstrip("# ").strip()
            keep = title in headings
        if keep:
            buf.append(line)
    return "\n".join(buf)

# â”€â”€ Prompt Generator â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def get_system_prompt() -> str:
    # A) OS ãƒ«ãƒ¼ãƒ«
    rules = read(DOCS / "base_os_rules.md")

    # B) DSL
    dsl_readme = (DSL / "README.md").read_text(encoding="utf-8") if (DSL / "README.md").exists() else ""
    dsl_lines: list[str] = []
    for raw in (DSL / "integrated_dsl.jsonl").read_text(encoding="utf-8").splitlines():
        try:
            item = json.loads(raw)
            name, desc = item.get("name"), item.get("description")
            if name and desc:
                dsl_lines.append(f"- **{name}**: {desc}")
        except Exception:
            continue
    dsl_block = "\n".join([dsl_readme.strip()] + dsl_lines if dsl_readme else dsl_lines)

    # C) Project Definition
    proj = extract_section(DOCS / "project_definition.md", ["ç›®çš„", "ã‚´ãƒ¼ãƒ«", "Kai Support ã®å½¹å‰²"])

    # D) Architecture Overview
    arch = extract_section(DOCS / "architecture_overview.md", ["PoCæ§‹æˆ", "é‹ç”¨ãƒ•ãƒ­ãƒ¼"])

    return "\n\n".join([rules, dsl_block, proj, arch])

# â”€â”€ Streamlit UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="Kai Chat", page_icon="ğŸ’¬")
st.title("ğŸ’¬ Kai - GPTãƒãƒ£ãƒƒãƒˆ")

if "history" not in st.session_state:
    st.session_state["history"] = []

# å±¥æ­´è¡¨ç¤º
for msg in st.session_state["history"]:
    st.chat_message("user" if msg["role"] == "user" else "assistant").markdown(msg["content"])

# å…¥åŠ›æ¬„ï¼ˆå¸¸ã«æœ€ä¸‹æ®µï¼‰
user_input = st.chat_input("ã‚ãªãŸã®ç™ºè¨€")  # â† ã“ã“ã ã‘å¤‰æ›´

if user_input:
    try:
        system_prompt = get_system_prompt()
        msgs = [{"role": "system", "content": system_prompt}] + \
               st.session_state["history"] + \
               [{"role": "user", "content": user_input}]
        reply = openai.chat.completions.create(
            model="gpt-3.5-turbo",
            messages=msgs
        ).choices[0].message.content

        st.session_state["history"].extend([
            {"role": "user", "content": user_input},
            {"role": "assistant", "content": reply},
        ])
        # rerun ä¸è¦ã€‚chat_input ã¯é€ä¿¡å¾Œã«è‡ªå‹•ã‚¯ãƒªã‚¢ã•ã‚Œã‚‹
    except Exception as e:
        st.error(f"âŒ OpenAI å‘¼ã³å‡ºã—å¤±æ•—: {e}")
        traceback.print_exc()
